
---

## 🔐 15. Working with AWS and Cloud in PySpark

As big data solutions move increasingly to the cloud, it's essential for PySpark data engineers to be proficient in working with **AWS services** like **S3**, **EMR**, **Secrets Manager**, and **Snowflake**.

---

### 📥 Reading/Writing Data to/from Amazon S3

**Amazon S3** is the most commonly used cloud storage service for big data. It is used to store raw, intermediate, and final data outputs in PySpark pipelines.

#### ✅ Read from S3:

```python
df = spark.read.parquet("s3://my-bucket/input-data/")
```

#### ✅ Write to S3:

```python
df.write.mode("overwrite").parquet("s3://my-bucket/output-data/")
```

**Supported formats**: Parquet, CSV, JSON, ORC, Delta, etc.

#### 🔐 Permissions:

* Ensure your Spark cluster or notebook has access to S3 via IAM Role or AWS credentials.
* You can also set the credentials using:

```python
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "<ACCESS_KEY>")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "<SECRET_KEY>")
```

> 🔎 Prefer IAM roles for secure access — avoid hardcoding credentials.

---

### ❄️ Connecting to Snowflake Using Connectors

**Snowflake** is a cloud data warehouse. PySpark can connect to Snowflake using the **Snowflake Spark Connector**.

#### ✅ Required JARs:

You need the Snowflake JDBC and Spark connector JARs when initializing your Spark session.

#### ✅ Sample code to read from Snowflake:

```python
sfOptions = {
  "sfURL": "<account>.snowflakecomputing.com",
  "sfUser": "username",
  "sfPassword": "password",
  "sfDatabase": "MY_DB",
  "sfSchema": "PUBLIC",
  "sfWarehouse": "COMPUTE_WH",
  "sfRole": "SYSADMIN"
}

df = spark.read \
    .format("snowflake") \
    .options(**sfOptions) \
    .option("dbtable", "CUSTOMER_DATA") \
    .load()
```

#### ✅ Writing to Snowflake:

```python
df.write \
    .format("snowflake") \
    .options(**sfOptions) \
    .option("dbtable", "PROCESSED_DATA") \
    .mode("overwrite") \
    .save()
```

---

### 🔐 Using AWS Secrets Manager for Credentials

Avoid hardcoding sensitive information (like Snowflake credentials or API keys). Instead, store them securely in **AWS Secrets Manager** and fetch them in your PySpark job.

#### ✅ Example (Python code):

```python
import boto3
import json

client = boto3.client("secretsmanager", region_name="us-east-1")
response = client.get_secret_value(SecretId="snowflake_credentials")
secrets = json.loads(response["SecretString"])

sf_user = secrets["username"]
sf_password = secrets["password"]
```

You can pass these values dynamically to your Spark config or connector.

> ✅ **Best Practice**: Always retrieve credentials at runtime using secure APIs.

---

### ⚙️ Running Jobs on AWS EMR

**EMR (Elastic MapReduce)** is a fully managed big data platform by AWS. It supports Spark, Hadoop, Hive, and more.

#### ✅ Key EMR features:

* Auto-scaling clusters
* Spot instances for cost savings
* Integration with S3, Glue, RDS, Snowflake, Athena, etc.
* Cluster bootstrapping and bootstrap actions
* Works well with Airflow and Step Functions

#### ✅ Running a PySpark job on EMR:

1. Submit a job using `spark-submit`
2. Schedule jobs using Airflow or AWS Step Functions
3. Use EMR on EKS or Serverless for even greater flexibility

```bash
spark-submit \
  --deploy-mode cluster \
  --master yarn \
  s3://my-bucket/scripts/my_spark_job.py
```

---

### 🔄 AWS Glue vs EMR: Comparison

| Feature          | AWS Glue                          | AWS EMR                                   |
| ---------------- | --------------------------------- | ----------------------------------------- |
| Type             | Serverless ETL service            | Managed big data cluster service          |
| Language Support | Python (PySpark), Scala (limited) | PySpark, Scala, Java, Hive, Presto        |
| Use Case         | Lightweight ETL pipelines         | Complex, compute-heavy Spark jobs         |
| Job Start Time   | Slower (cold start issues)        | Faster (persistent clusters)              |
| Auto Scaling     | Yes                               | Yes (needs setup)                         |
| Cost Model       | Per-second pricing                | Pay per instance/hour                     |
| Monitoring       | CloudWatch                        | CloudWatch + Spark UI                     |
| Customization    | Limited                           | Highly customizable (AMI, bootstrap, etc) |
| Performance      | Moderate                          | High (you control the cluster)            |

> 🔎 Choose **Glue** for simple ETL jobs where convenience and cost-efficiency matter.
> 💪 Choose **EMR** for **complex workloads** or when you need full control and performance.

---

### ✅ Summary

* Read/write data to **Amazon S3** using simple `.read()` and `.write()` methods.
* Use **Snowflake Spark Connector** for seamless data warehouse integration.
* Secure credentials using **AWS Secrets Manager** instead of hardcoding.
* Run scalable Spark jobs on **AWS EMR** clusters or use **Glue** for lighter, serverless jobs.
* Understand the tradeoffs between Glue (serverless) and EMR (custom, powerful).

---
