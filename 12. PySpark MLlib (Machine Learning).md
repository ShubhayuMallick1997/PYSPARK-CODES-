
---

### ðŸ§  12. PySpark MLlib (Machine Learning)

---

#### ðŸ”¹ What is MLlib?

**MLlib** is Apache Sparkâ€™s scalable machine learning library. It supports large-scale data processing for common ML tasks like classification, regression, clustering, recommendation, and more â€” all distributed across a Spark cluster.

It is built on top of Spark DataFrames and designed to work efficiently on big data.

---

### ðŸ”¸ MLlib vs scikit-learn

| Feature           | MLlib (PySpark)                          | scikit-learn                      |
| ----------------- | ---------------------------------------- | --------------------------------- |
| Scale             | Distributed across clusters              | Runs on a single machine          |
| Language Support  | Scala, Python (PySpark), Java            | Python only                       |
| Integration       | Built into Spark (works with DataFrames) | Works with NumPy/Pandas           |
| Speed on Big Data | Very fast on large datasets              | May crash or lag on huge datasets |
| Ease of Use       | Slightly more verbose                    | More intuitive for beginners      |

**In short:**

* Use **scikit-learn** for small to medium data on a single machine.
* Use **MLlib** when working with **huge datasets in distributed environments** like Hadoop or Spark clusters.

---

### ðŸ”¹ Feature Transformers in MLlib

Before feeding data into a machine learning model, you need to convert it into numerical format. MLlib provides many **transformers** to help with that.

#### âœ… StringIndexer

Converts categorical string columns into numerical indices.

```python
from pyspark.ml.feature import StringIndexer

indexer = StringIndexer(inputCol="gender", outputCol="gender_index")
df_indexed = indexer.fit(df).transform(df)
```

---

#### âœ… VectorAssembler

Combines multiple feature columns into a single vector column â€” required by all ML models.

```python
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(
    inputCols=["age", "salary", "gender_index"],
    outputCol="features"
)
df_final = assembler.transform(df_indexed)
```

---

### ðŸ”¹ Building a ML Pipeline in PySpark

MLlib allows chaining multiple steps together using a **Pipeline**, similar to scikit-learnâ€™s `Pipeline`.

Each step (like StringIndexer, VectorAssembler, or a model) is treated as a `Stage`.

```python
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression

pipeline = Pipeline(stages=[indexer, assembler, LogisticRegression(labelCol="label", featuresCol="features")])
model = pipeline.fit(train_df)
predictions = model.transform(test_df)
```

This makes preprocessing and training **modular and reusable**.

---

### ðŸ”¹ Classification Example

Using Logistic Regression:

```python
from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(featuresCol="features", labelCol="label")
lr_model = lr.fit(training_data)
predictions = lr_model.transform(test_data)
```

Other classification algorithms in MLlib include:

* DecisionTreeClassifier
* RandomForestClassifier
* GBTClassifier
* MultilayerPerceptronClassifier (Neural Net)

---

### ðŸ”¹ Regression Example

Using Linear Regression:

```python
from pyspark.ml.regression import LinearRegression

lr = LinearRegression(featuresCol="features", labelCol="label")
model = lr.fit(training_data)
predictions = model.transform(test_data)
```

Other regression models:

* DecisionTreeRegressor
* RandomForestRegressor
* GBTRegressor

---

### ðŸ”¹ Hyperparameter Tuning with CrossValidator

Like GridSearchCV in scikit-learn, PySpark offers `CrossValidator` for parameter tuning.

```python
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import BinaryClassificationEvaluator

paramGrid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \
    .build()

evaluator = BinaryClassificationEvaluator(labelCol="label")

cv = CrossValidator(estimator=lr,
                    estimatorParamMaps=paramGrid,
                    evaluator=evaluator,
                    numFolds=3)

cv_model = cv.fit(training_data)
best_model = cv_model.bestModel
```

This performs k-fold cross-validation and selects the best combination of hyperparameters.

---

### âœ… Summary

* MLlib is Sparkâ€™s machine learning library for big data.
* It supports transformers like `StringIndexer` and `VectorAssembler` to prepare data.
* ML Pipelines organize the full workflow from preprocessing to prediction.
* It supports all major classification and regression models.
* Use `CrossValidator` for scalable hyperparameter tuning.

---


