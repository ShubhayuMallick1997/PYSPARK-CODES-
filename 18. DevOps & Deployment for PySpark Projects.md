
---

## 🧰 18. DevOps & Deployment for PySpark Projects

Modern data engineering is not just about writing transformations — it’s also about **collaborating via Git**, **automating deployments**, and **monitoring job health** in production. This section covers best practices and tools for PySpark deployment pipelines.

---

### ✅ 1. Git Version Control in PySpark Projects

**Git** is essential for managing PySpark code, collaboration, and version control. In a professional setup:

* Each feature or bug fix is developed in a separate **Git branch**.
* Teams use **Pull Requests (PRs)** for code reviews.
* All changes are merged into the `main` or `develop` branch after approval.

#### 🔸 Best Practices:

* Use a clear repo structure:

  ```
  pyspark-project/
  ├── scripts/
  │   ├── job1.py
  │   └── job2.py
  ├── config/
  ├── requirements.txt
  ├── README.md
  └── tests/
  ```
* Commit messages should be atomic and descriptive:

  ```
  git commit -m "Add transformation to filter inactive users"
  ```
* Use `.gitignore` to exclude logs, local files, `.idea/`, `.pyc` files, etc.

---

### 🔄 2. Creating CI/CD Pipelines with Jenkins

**Jenkins** is a popular tool for automating **Continuous Integration (CI)** and **Continuous Deployment (CD)** in PySpark projects.

#### 🔹 What CI/CD does:

* Runs tests automatically on every push or PR.
* Builds and packages the PySpark job.
* Deploys it to a staging or production environment.
* Sends alerts for success/failure.

#### 🔸 Example Jenkins Pipeline (Jenkinsfile):

```groovy
pipeline {
    agent any

    stages {
        stage('Checkout') {
            steps {
                git branch: 'main', url: 'https://github.com/your-org/pyspark-project'
            }
        }

        stage('Install Dependencies') {
            steps {
                sh 'pip install -r requirements.txt'
            }
        }

        stage('Run Tests') {
            steps {
                sh 'pytest tests/'
            }
        }

        stage('Deploy to EMR') {
            steps {
                sh 'aws emr add-steps --cluster-id j-XXXX --steps Type=Spark,Name="Run PySpark",Args=[spark-submit,s3://bucket/scripts/main.py]'
            }
        }
    }
}
```

---

### 📦 3. Packaging PySpark Projects

To make your PySpark job portable and production-ready, **package your code** with dependencies.

#### 🔹 Options:

* **Python packages** (`setup.py`) — for reusable libraries

* **ZIP packages** for Spark submit:

  ```bash
  zip -r myjob.zip scripts/ config/ requirements.txt
  spark-submit --py-files myjob.zip main.py
  ```

* Use **Docker** to containerize your job for reproducibility across environments.

#### 🔸 Example `setup.py`:

```python
from setuptools import setup, find_packages

setup(
    name="customer360",
    version="0.1",
    packages=find_packages(),
    install_requires=["pyspark", "boto3", "snowflake-connector-python"]
)
```

---

### 🗓️ 4. Job Scheduling and Monitoring

After deployment, you need to **run jobs on schedule and monitor** their performance.

#### ✅ Common Schedulers:

* **Airflow**: Advanced DAG-based scheduler (recommended)
* **AWS Step Functions**: For orchestrating EMR workflows
* **Crontab**: For simple bash-based scheduling
* **Jenkins**: Can also be configured to run jobs periodically

#### 🔍 Monitoring Tools:

* **Airflow UI**: Shows task success/failure, duration
* **Spark UI**: Detailed view of stages, tasks, memory
* **AWS CloudWatch / EMR logs**: Logs for production monitoring
* **Slack/Webhook Alerts**: Send job failures directly to your team

---

### ✅ Summary

| Area                 | Tool / Practice               | Purpose                                    |
| -------------------- | ----------------------------- | ------------------------------------------ |
| Version Control      | Git + GitHub/GitLab           | Manage code collaboratively                |
| CI/CD                | Jenkins, GitHub Actions       | Automate tests, builds, and deployments    |
| Packaging            | setup.py, ZIP, Docker         | Make Spark jobs portable and reproducible  |
| Job Scheduling       | Airflow, Jenkins, StepFunc    | Run jobs on time with defined dependencies |
| Monitoring & Logging | Spark UI, CloudWatch, Airflow | Debug and track job health in production   |

---

