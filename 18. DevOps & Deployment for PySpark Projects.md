
---

## ğŸ§° 18. DevOps & Deployment for PySpark Projects

Modern data engineering is not just about writing transformations â€” itâ€™s also about **collaborating via Git**, **automating deployments**, and **monitoring job health** in production. This section covers best practices and tools for PySpark deployment pipelines.

---

### âœ… 1. Git Version Control in PySpark Projects

**Git** is essential for managing PySpark code, collaboration, and version control. In a professional setup:

* Each feature or bug fix is developed in a separate **Git branch**.
* Teams use **Pull Requests (PRs)** for code reviews.
* All changes are merged into the `main` or `develop` branch after approval.

#### ğŸ”¸ Best Practices:

* Use a clear repo structure:

  ```
  pyspark-project/
  â”œâ”€â”€ scripts/
  â”‚   â”œâ”€â”€ job1.py
  â”‚   â””â”€â”€ job2.py
  â”œâ”€â”€ config/
  â”œâ”€â”€ requirements.txt
  â”œâ”€â”€ README.md
  â””â”€â”€ tests/
  ```
* Commit messages should be atomic and descriptive:

  ```
  git commit -m "Add transformation to filter inactive users"
  ```
* Use `.gitignore` to exclude logs, local files, `.idea/`, `.pyc` files, etc.

---

### ğŸ”„ 2. Creating CI/CD Pipelines with Jenkins

**Jenkins** is a popular tool for automating **Continuous Integration (CI)** and **Continuous Deployment (CD)** in PySpark projects.

#### ğŸ”¹ What CI/CD does:

* Runs tests automatically on every push or PR.
* Builds and packages the PySpark job.
* Deploys it to a staging or production environment.
* Sends alerts for success/failure.

#### ğŸ”¸ Example Jenkins Pipeline (Jenkinsfile):

```groovy
pipeline {
    agent any

    stages {
        stage('Checkout') {
            steps {
                git branch: 'main', url: 'https://github.com/your-org/pyspark-project'
            }
        }

        stage('Install Dependencies') {
            steps {
                sh 'pip install -r requirements.txt'
            }
        }

        stage('Run Tests') {
            steps {
                sh 'pytest tests/'
            }
        }

        stage('Deploy to EMR') {
            steps {
                sh 'aws emr add-steps --cluster-id j-XXXX --steps Type=Spark,Name="Run PySpark",Args=[spark-submit,s3://bucket/scripts/main.py]'
            }
        }
    }
}
```

---

### ğŸ“¦ 3. Packaging PySpark Projects

To make your PySpark job portable and production-ready, **package your code** with dependencies.

#### ğŸ”¹ Options:

* **Python packages** (`setup.py`) â€” for reusable libraries

* **ZIP packages** for Spark submit:

  ```bash
  zip -r myjob.zip scripts/ config/ requirements.txt
  spark-submit --py-files myjob.zip main.py
  ```

* Use **Docker** to containerize your job for reproducibility across environments.

#### ğŸ”¸ Example `setup.py`:

```python
from setuptools import setup, find_packages

setup(
    name="customer360",
    version="0.1",
    packages=find_packages(),
    install_requires=["pyspark", "boto3", "snowflake-connector-python"]
)
```

---

### ğŸ—“ï¸ 4. Job Scheduling and Monitoring

After deployment, you need to **run jobs on schedule and monitor** their performance.

#### âœ… Common Schedulers:

* **Airflow**: Advanced DAG-based scheduler (recommended)
* **AWS Step Functions**: For orchestrating EMR workflows
* **Crontab**: For simple bash-based scheduling
* **Jenkins**: Can also be configured to run jobs periodically

#### ğŸ” Monitoring Tools:

* **Airflow UI**: Shows task success/failure, duration
* **Spark UI**: Detailed view of stages, tasks, memory
* **AWS CloudWatch / EMR logs**: Logs for production monitoring
* **Slack/Webhook Alerts**: Send job failures directly to your team

---

### âœ… Summary

| Area                 | Tool / Practice               | Purpose                                    |
| -------------------- | ----------------------------- | ------------------------------------------ |
| Version Control      | Git + GitHub/GitLab           | Manage code collaboratively                |
| CI/CD                | Jenkins, GitHub Actions       | Automate tests, builds, and deployments    |
| Packaging            | setup.py, ZIP, Docker         | Make Spark jobs portable and reproducible  |
| Job Scheduling       | Airflow, Jenkins, StepFunc    | Run jobs on time with defined dependencies |
| Monitoring & Logging | Spark UI, CloudWatch, Airflow | Debug and track job health in production   |

---

