
---

## ðŸ“š 19. Advanced Topics in PySpark

As data engineering evolves, traditional data lakes are becoming **lakehouses**â€”combining the **scalability of data lakes** with the **ACID guarantees and query performance** of data warehouses. This section dives into technologies and techniques to handle **real-world, large-scale, production-grade data systems**.

---

### ðŸ§Š Delta Lake with PySpark

#### ðŸ”¹ What is Delta Lake?

**Delta Lake** is an open-source storage layer that brings **ACID transactions**, **schema enforcement**, and **time travel** to big data lakes, especially on top of **Apache Parquet files**.

#### âœ… Key Features:

* **ACID transactions** â€” Safe concurrent reads/writes
* **Schema evolution & enforcement**
* **Time travel** â€” Query data as of a previous timestamp/version
* **Efficient updates & deletes** using Delta logs

#### ðŸ”¸ PySpark Code Example:

```python
# Writing data to Delta format
df.write.format("delta").mode("overwrite").save("s3://my-bucket/delta/customers")

# Reading Delta Lake table
df = spark.read.format("delta").load("s3://my-bucket/delta/customers")

# Time travel
df_old = spark.read.format("delta").option("versionAsOf", 5).load("s3://...")

# Deleting rows
from delta.tables import DeltaTable
deltaTable = DeltaTable.forPath(spark, "s3://my-bucket/delta/customers")
deltaTable.delete("status = 'inactive'")
```

> ðŸ’¡ **Delta Lake** is native in **Databricks**, but also works in open-source Spark with Delta JARs.

---

### ðŸ§Š Apache Iceberg Basics

#### ðŸ”¹ What is Apache Iceberg?

**Apache Iceberg** is a high-performance, **table format** for huge analytic datasets. Like Delta, it enables **SQL on data lakes** while supporting:

* Schema evolution
* Partition pruning
* Hidden partitioning
* Time travel and rollback

#### âœ… Iceberg vs Delta:

| Feature            | Delta Lake           | Apache Iceberg             |
| ------------------ | -------------------- | -------------------------- |
| Format             | Parquet + Delta logs | Parquet + Iceberg metadata |
| Vendor             | Databricks (primary) | Netflix, Apache, Snowflake |
| Schema Evolution   | Supported            | Supported                  |
| Deletes & Updates  | Yes                  | Yes                        |
| Time Travel        | Yes                  | Yes                        |
| Engine Integration | Spark, Presto, Flink | Spark, Trino, Hive, Flink  |

> ðŸ“Œ Use Iceberg if you're using engines like **Trino**, **Presto**, or **Flink** along with Spark.

---

### ðŸ  Lakehouse Architecture

The **Lakehouse** is a hybrid architecture that combines features of **Data Lakes (e.g., S3, HDFS)** and **Data Warehouses (e.g., Snowflake, Redshift)**.

#### ðŸ”¹ Key Components:

1. **Storage Layer** â€“ Raw, structured, and unstructured data (S3, ADLS)
2. **Metadata Layer** â€“ Table format (Delta, Iceberg, Hudi)
3. **Processing Layer** â€“ PySpark, SQL, Presto, Flink
4. **Query Layer** â€“ BI tools (Tableau, Power BI)

#### âœ… Benefits:

* Unified architecture: One copy of data for batch & analytics
* Lower cost than traditional data warehouses
* Handles structured & unstructured data
* Scales with cloud-native tools

> âœ… Databricks and AWS Glue support Lakehouse natively using Delta/Iceberg.

---

### ðŸ” Z-Ordering and `OPTIMIZE` for Large Datasets

#### ðŸ”¹ What is Z-Ordering?

**Z-Ordering** is a multi-dimensional clustering technique used in Delta Lake to **co-locate related information in the same data files**, improving **query performance**.

#### ðŸ”¸ Example:

```sql
OPTIMIZE customers ZORDER BY (customer_id, region)
```

It reorganizes the data so that queries using `WHERE customer_id = ...` will scan fewer files.

> âš ï¸ Requires Delta Lake and Databricks or compatible engines.

---

#### ðŸ”¹ What is `OPTIMIZE`?

Used in Delta to compact small files into larger ones â€” critical for performance and cost savings.

```sql
OPTIMIZE sales_data
```

Improves:

* Query performance
* File management on S3
* Cost for scanning with engines like Athena/Presto

---

### âš™ï¸ Performance Tuning for Big Jobs

Handling large-scale jobs (terabytes of data) requires tuning at various levels:

#### âœ… Spark-Level Tips:

| Area           | Technique                                    |
| -------------- | -------------------------------------------- |
| Partitioning   | Use `.repartition()` or `.coalesce()` wisely |
| Caching        | Use `.cache()` only for reused DataFrames    |
| Join strategy  | Use broadcast joins for small tables         |
| Shuffle tuning | `spark.sql.shuffle.partitions`               |
| Memory tuning  | Configure executor memory and cores          |
| File format    | Use **Parquet** or **Delta** over CSV        |

#### ðŸ”¸ Example Configs:

```python
spark.conf.set("spark.sql.shuffle.partitions", "200")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 50 * 1024 * 1024)
```

> âœ… Use `.explain()` and Spark UI to monitor performance.

---

### âœ… Summary

| Topic             | Purpose                                              |
| ----------------- | ---------------------------------------------------- |
| Delta Lake        | ACID, schema evolution, updates on data lakes        |
| Apache Iceberg    | Vendor-agnostic table format for big data            |
| Lakehouse         | Unified architecture for lakes + warehouse analytics |
| Z-Ordering        | Cluster related data for faster querying             |
| OPTIMIZE          | Compacts files for performance gains                 |
| Tuning Techniques | Partitioning, broadcast joins, memory configs        |

---

