
---

## ü™Ñ 16. Airflow Integration with PySpark

---

### üîπ What is Apache Airflow?

**Apache Airflow** is an open-source workflow orchestration tool used to programmatically author, schedule, and monitor workflows (called DAGs ‚Äì Directed Acyclic Graphs).

Airflow is widely used to **orchestrate PySpark jobs** in data engineering projects, especially when:

* Multiple tasks/jobs need to run in a defined sequence
* You want to automate Spark job execution on a schedule (e.g., daily at 5 AM)
* Monitoring and retries are important for stability
* Integration with cloud services like **AWS EMR**, **S3**, **Snowflake**, etc., is required

---

### ‚úÖ Designing Airflow DAGs for PySpark Jobs

Airflow DAGs are written in Python, where each **task** represents a unit of work. A PySpark job can be triggered via:

* **PythonOperator** (if Spark job is in the same environment)
* **BashOperator** (to run `spark-submit`)
* **EMR Operators** (if running on AWS EMR)

#### Basic DAG Structure:

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'shubhayu',
    'start_date': datetime(2024, 1, 1),
    'retries': 1
}

with DAG('pyspark_daily_job',
         default_args=default_args,
         schedule_interval='0 5 * * *',  # Every day at 5 AM
         catchup=False) as dag:

    spark_task = BashOperator(
        task_id='run_pyspark_job',
        bash_command='spark-submit /home/airflow/scripts/my_job.py'
    )
```

You can define **dependencies** like:

```python
task1 >> task2  # Run task2 only after task1
```

---

### üß© Using EMR Operators (on AWS)

When your Spark jobs run on **AWS EMR**, use Airflow‚Äôs **EMR operators** to spin up clusters, submit steps, and terminate them ‚Äî all through the DAG.

#### Key Operators:

* `EmrCreateJobFlowOperator` ‚Üí Launch EMR cluster
* `EmrAddStepsOperator` ‚Üí Submit Spark jobs as steps
* `EmrStepSensor` ‚Üí Monitor step completion
* `EmrTerminateJobFlowOperator` ‚Üí Shut down the cluster

#### Example:

```python
from airflow.providers.amazon.aws.operators.emr import EmrCreateJobFlowOperator, EmrTerminateJobFlowOperator, EmrAddStepsOperator
from airflow.providers.amazon.aws.sensors.emr import EmrStepSensor

JOB_FLOW_OVERRIDES = {
    "Name": "PySparkCluster",
    "ReleaseLabel": "emr-6.12.0",
    "Applications": [{"Name": "Spark"}],
    "Instances": {
        "InstanceGroups": [
            {"Name": "Master", "InstanceType": "m5.xlarge", "InstanceCount": 1},
            {"Name": "Core", "InstanceType": "m5.xlarge", "InstanceCount": 2}
        ],
        "KeepJobFlowAliveWhenNoSteps": True,
        "TerminationProtected": False,
    },
    "JobFlowRole": "EMR_EC2_DefaultRole",
    "ServiceRole": "EMR_DefaultRole"
}

with DAG('emr_spark_job', schedule_interval='@daily', start_date=datetime(2024, 1, 1), catchup=False) as dag:

    create_cluster = EmrCreateJobFlowOperator(
        task_id='create_emr_cluster',
        job_flow_overrides=JOB_FLOW_OVERRIDES,
        aws_conn_id='aws_default'
    )

    add_step = EmrAddStepsOperator(
        task_id='add_spark_step',
        job_flow_id="{{ task_instance.xcom_pull(task_ids='create_emr_cluster', key='return_value') }}",
        steps=[{
            'Name': 'Run PySpark',
            'ActionOnFailure': 'CONTINUE',
            'HadoopJarStep': {
                'Jar': 'command-runner.jar',
                'Args': ['spark-submit', 's3://my-bucket/scripts/job.py']
            }
        }],
        aws_conn_id='aws_default'
    )

    monitor_step = EmrStepSensor(
        task_id='watch_step',
        job_flow_id="{{ task_instance.xcom_pull(task_ids='create_emr_cluster', key='return_value') }}",
        step_id="{{ task_instance.xcom_pull(task_ids='add_spark_step', key='return_value')[0] }}",
        aws_conn_id='aws_default'
    )

    terminate_cluster = EmrTerminateJobFlowOperator(
        task_id='terminate_cluster',
        job_flow_id="{{ task_instance.xcom_pull(task_ids='create_emr_cluster', key='return_value') }}",
        aws_conn_id='aws_default'
    )

    create_cluster >> add_step >> monitor_step >> terminate_cluster
```

---

### üêç Triggering PySpark Jobs via PythonOperator

If your PySpark job is callable as a Python function (e.g., in Databricks or on the same host), use `PythonOperator`:

```python
from airflow.operators.python import PythonOperator

def run_my_pyspark():
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.appName("AirflowPySpark").getOrCreate()
    df = spark.read.csv("s3://mybucket/file.csv", header=True)
    df.show()

spark_python_task = PythonOperator(
    task_id='pyspark_python_func',
    python_callable=run_my_pyspark
)
```

---

### ‚úÖ Best Practices for Airflow + PySpark Integration

* **Use EMR for large jobs**, and terminate clusters after completion to save cost.
* **Separate your DAG logic from PySpark logic** ‚Äî keep your Spark scripts modular.
* **Use Airflow variables & connections** to manage dynamic inputs like S3 paths or credentials.
* Monitor DAGs using Airflow UI and set up **alerts** for failures.
* Store logs and checkpoints in **S3 or CloudWatch** for audit and debugging.

---

### ‚úÖ Summary

* Airflow helps automate and orchestrate PySpark pipelines on a schedule or event.
* Use **BashOperator** for local `spark-submit`, or **PythonOperator** for embedded logic.
* Use **EMR Operators** to run PySpark on AWS EMR clusters in a scalable and cost-effective way.
* Monitor, retry, and chain PySpark jobs efficiently using DAGs.

---

