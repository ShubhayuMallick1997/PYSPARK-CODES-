
---

# ‚öôÔ∏è 10. Partitioning & Performance Optimization

---

## üîπ Why Performance Optimization Matters in PySpark

PySpark applications often process **huge volumes of data** distributed across clusters. If not optimized, your Spark jobs can:

* Consume excessive memory or CPU
* Run slowly or fail due to resource exhaustion
* Cost more on cloud platforms

Optimizing partitioning, caching, and resource usage is essential for **faster, cost-effective, and reliable pipelines**.

---

## ‚úÖ 1. Partitioning in Spark

**Partitioning** determines how data is split across the cluster. More partitions = more parallelism. However, too many small partitions or too few large ones can degrade performance.

### Default Partitioning:

* Based on the number of cores (`spark.default.parallelism`)
* Can be influenced by `repartition()` and `coalesce()`

### Example: Repartition

```python
df = df.repartition(100)  # Increase number of partitions
```

### Example: Coalesce

```python
df = df.coalesce(10)  # Reduce partitions efficiently (avoids shuffle)
```

> üîÅ Use `repartition()` when increasing partitions (e.g., after joins), and `coalesce()` when decreasing before writing to storage.

---

## ‚úÖ 2. Broadcast Joins

When one DataFrame is **very small**, broadcasting it to all executors avoids expensive shuffles.

```python
from pyspark.sql.functions import broadcast

df_large.join(broadcast(df_small), on="id").show()
```

> ‚úÖ Great for joining reference tables (e.g., country codes, currency rates).

---

## ‚úÖ 3. Persisting and Caching Data

When using the same DataFrame multiple times in a pipeline (e.g., for multiple transformations or joins), **cache or persist it**.

```python
df.cache()  # Stores in memory
df.persist(StorageLevel.MEMORY_AND_DISK)
```

> üöÄ Useful in iterative computations like ML or Graph algorithms.

---

## ‚úÖ 4. File Format Optimization

Use **Parquet** or **ORC** for reading/writing data ‚Äî they are columnar, compressed, and optimized for Spark.

```python
df.write.parquet("path/to/output")
```

Also, avoid reading large CSV/JSON files unnecessarily ‚Äî they are slower and not compressed.

---

## ‚úÖ 5. Column Pruning & Predicate Pushdown

When working with Parquet or ORC, Spark automatically applies:

* **Column pruning**: Only reads necessary columns
* **Predicate pushdown**: Applies filters at the storage level

So, always **select only required columns** and **apply filters early**:

```python
df.select("id", "amount").filter(df["amount"] > 1000)
```

---

## ‚úÖ 6. Avoid Using UDFs Where Possible

Prefer built-in Spark functions like `when()`, `col()`, `concat()`, `upper()`, etc. These are optimized and executed natively.

---

## ‚úÖ 7. Tune Spark Configurations

Fine-tune Spark properties based on workload:

```python
spark.conf.set("spark.sql.shuffle.partitions", "200")
spark.conf.set("spark.executor.memory", "4g")
spark.conf.set("spark.executor.cores", "4")
```

---

## ‚úÖ 8. Skew Handling Techniques

When data is unevenly distributed (skewed), a few partitions may slow down the entire job.

### Solutions:

* Use **salting** (add a random key to distribute skewed keys)
* Use **broadcast joins** to avoid shuffles
* Filter out skewed keys and process them separately

---

## ‚úÖ Summary Table

| Technique               | Purpose                                  |
| ----------------------- | ---------------------------------------- |
| `repartition()`         | Increase number of partitions            |
| `coalesce()`            | Reduce partitions without full shuffle   |
| `broadcast()`           | Optimize joins with small tables         |
| `cache()` / `persist()` | Store intermediate results               |
| Column pruning          | Read only necessary columns from disk    |
| Predicate pushdown      | Filter data at storage level             |
| Avoid UDFs              | Use optimized built-in functions         |
| Tune configs            | Control memory, parallelism, and shuffle |
| Handle skew             | Use salting, broadcast, or custom logic  |

---
Absolutely! Here's a clear and professional breakdown of the advanced Spark performance concepts you've mentioned, all written in **clean Markdown** format suitable for technical portfolios, documentation, or study notes.

---

# ‚öôÔ∏è PySpark Performance Essentials

---

## üîÅ Repartition vs Coalesce

### üîπ `repartition()`

* **Used to increase** the number of partitions
* **Triggers a full shuffle** of data across the cluster
* **Ensures even distribution** of records
* Suitable when you're preparing for wide transformations (e.g., joins)

```python
df_repart = df.repartition(100)
```

### üîπ `coalesce()`

* **Used to reduce** the number of partitions
* **Avoids full shuffle** (uses narrow transformation)
* More efficient than `repartition()` for downsizing
* Best used before writing data (e.g., fewer output files)

```python
df_coal = df.coalesce(10)
```

> ‚úÖ **Rule of Thumb**:
> Use `repartition()` to **increase partitions**, and `coalesce()` to **reduce them** before saving.

---

## üß† Caching and Persistence

### üîπ cache()

* Stores the DataFrame in **memory**.
* Ideal for reusing the same dataset multiple times in your pipeline.
* Easy to apply: `df.cache()`

### üîπ persist()

* More flexible: stores data in **memory, disk, or both**.
* Use when data doesn‚Äôt fit into memory completely.

```python
from pyspark import StorageLevel
df.persist(StorageLevel.MEMORY_AND_DISK)
```
---

## ‚öôÔ∏è Catalyst Optimizer and Tungsten Engine

### üîπ Catalyst Optimizer

* Spark‚Äôs **rule-based and cost-based** query optimizer for DataFrames and SQL.
* Optimizes logical query plans via:

  * Predicate pushdown
  * Constant folding
  * Null propagation
  * Filter reordering

**Example:**

```python
df.select("name").filter("age > 30")
```

Catalyst rearranges operations for optimal performance.

### üîπ Tungsten Execution Engine

* Spark‚Äôs **physical execution engine** for memory management and binary processing.
* Includes:

  * Whole-stage code generation
  * Off-heap memory use
  * Cache-friendly execution

> ‚ö†Ô∏è Custom UDFs **disable Catalyst and Tungsten** benefits ‚Äì avoid them unless necessary.

---

## üß± Data Skew Handling

**Data Skew** occurs when certain keys appear more frequently than others, causing uneven load and slow performance in partitions.

### üîπ How to Detect Skew:

* Long stage durations
* Executors stuck on a few tasks
* Uneven partition sizes

### üîß Solutions:

1. **Salting Technique**

Add a random prefix to skewed keys to spread data more evenly.

```python
from pyspark.sql.functions import rand

df = df.withColumn("salt", (rand() * 10).cast("int"))
```

2. **Broadcast Join**

Use when one side of the join is small.

```python
df_large.join(broadcast(df_small), "key")
```

3. **Skewed Key Isolation**

Process skewed keys separately, then merge with the main data.

4. **Skew-aware Aggregation**

Use approximate aggregations or filter out high-frequency keys.

---

## üì¶ Best Practices for Large Datasets

### ‚úÖ General Tips

| Area               | Practice                                                                |
| ------------------ | ----------------------------------------------------------------------- |
| File Format        | Use **Parquet** or **ORC** for storage efficiency                       |
| Read Efficiency    | Select only needed columns, apply filters early                         |
| Partition Strategy | Tune number of partitions (based on data size and executor count)       |
| Caching            | Cache reused data intelligently                                         |
| Join Strategy      | Use **broadcast joins** when applicable                                 |
| Memory Management  | Use `persist()` for large iterative jobs                                |
| Avoid UDFs         | Use built-in functions like `when()`, `col()`, `regexp_extract()`       |
| Compression        | Use `snappy` with Parquet or ORC for faster reads                       |
| Monitoring         | Use Spark UI to identify stages with skew, long tasks, or memory issues |

---

---

### Repartition vs Coalesce

**repartition()** is used to increase the number of partitions in your DataFrame. It performs a full shuffle across the network to evenly distribute data. This is useful when the data is unbalanced or when you're preparing for expensive operations like joins or groupBy.

Example:

```python
df = df.repartition(100)
```

**coalesce()** is used to reduce the number of partitions, usually after some filtering or when writing data to files. It avoids a full shuffle and is more efficient than repartition when you're decreasing partitions.

Example:

```python
df = df.coalesce(10)
```

In short:

* Use `repartition()` to **increase** and balance partitions.
* Use `coalesce()` to **decrease** partitions, usually before saving data.

---

### Caching and Persistence

When you're using the same DataFrame multiple times in your code (for example, in joins, filters, or aggregations), it helps to **cache** or **persist** it to avoid recomputation.

* `cache()` stores data in memory.
* `persist()` lets you choose where to store (memory, disk, or both).

Example:

```python
df.cache()
# or
df.persist(StorageLevel.MEMORY_AND_DISK)
```

Use caching when your dataset fits in memory and you're going to reuse it several times in your job.

---

### Catalyst Optimizer and Tungsten Engine

**Catalyst Optimizer** is Spark's logical query planner. When you write transformations using DataFrames or SQL, Catalyst optimizes the query automatically. For example, it can:

* Remove unused columns
* Push filters closer to data sources
* Reorder operations for performance

**Tungsten Engine** is the physical execution engine that handles memory and computation. It uses:

* Whole-stage code generation
* Off-heap memory management
* Binary processing

Together, Catalyst and Tungsten make Spark fast. But keep in mind: if you use Python UDFs, these optimizations may not apply.

---

### Data Skew Handling

**Data skew** happens when some keys in your data are much more frequent than others. This can cause a few tasks to become very slow, while others finish quickly.

How to handle it:

1. **Salting**: Add random numbers to skewed keys to spread them across partitions.
2. **Broadcast joins**: If one table is small, broadcasting it avoids shuffling the large one.
3. **Isolate and handle skewed keys separately**: For example, filter out records with those keys, process them alone, then merge later.

---

### Best Practices for Large Datasets

* Use **Parquet** or **ORC** for reading/writing data instead of CSV or JSON. They‚Äôre much faster and more efficient.
* Select only the columns you need using `.select()`.
* Filter early in your pipeline to reduce data volume.
* Use **broadcast joins** when joining with small reference tables.
* Cache datasets you reuse, but avoid overusing it ‚Äî memory is limited.
* Avoid UDFs when possible. Use built-in Spark functions instead.
* Monitor jobs using the Spark UI to spot slow stages or memory issues.

---

