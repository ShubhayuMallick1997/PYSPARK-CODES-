
---

# ‚öôÔ∏è 10. Partitioning & Performance Optimization

---

## üîπ Why Performance Optimization Matters in PySpark

PySpark applications often process **huge volumes of data** distributed across clusters. If not optimized, your Spark jobs can:

* Consume excessive memory or CPU
* Run slowly or fail due to resource exhaustion
* Cost more on cloud platforms

Optimizing partitioning, caching, and resource usage is essential for **faster, cost-effective, and reliable pipelines**.

---

## ‚úÖ 1. Partitioning in Spark

**Partitioning** determines how data is split across the cluster. More partitions = more parallelism. However, too many small partitions or too few large ones can degrade performance.

### Default Partitioning:

* Based on the number of cores (`spark.default.parallelism`)
* Can be influenced by `repartition()` and `coalesce()`

### Example: Repartition

```python
df = df.repartition(100)  # Increase number of partitions
```

### Example: Coalesce

```python
df = df.coalesce(10)  # Reduce partitions efficiently (avoids shuffle)
```

> üîÅ Use `repartition()` when increasing partitions (e.g., after joins), and `coalesce()` when decreasing before writing to storage.

---

## ‚úÖ 2. Broadcast Joins

When one DataFrame is **very small**, broadcasting it to all executors avoids expensive shuffles.

```python
from pyspark.sql.functions import broadcast

df_large.join(broadcast(df_small), on="id").show()
```

> ‚úÖ Great for joining reference tables (e.g., country codes, currency rates).

---

## ‚úÖ 3. Persisting and Caching Data

When using the same DataFrame multiple times in a pipeline (e.g., for multiple transformations or joins), **cache or persist it**.

```python
df.cache()  # Stores in memory
df.persist(StorageLevel.MEMORY_AND_DISK)
```

> üöÄ Useful in iterative computations like ML or Graph algorithms.

---

## ‚úÖ 4. File Format Optimization

Use **Parquet** or **ORC** for reading/writing data ‚Äî they are columnar, compressed, and optimized for Spark.

```python
df.write.parquet("path/to/output")
```

Also, avoid reading large CSV/JSON files unnecessarily ‚Äî they are slower and not compressed.

---

## ‚úÖ 5. Column Pruning & Predicate Pushdown

When working with Parquet or ORC, Spark automatically applies:

* **Column pruning**: Only reads necessary columns
* **Predicate pushdown**: Applies filters at the storage level

So, always **select only required columns** and **apply filters early**:

```python
df.select("id", "amount").filter(df["amount"] > 1000)
```

---

## ‚úÖ 6. Avoid Using UDFs Where Possible

Prefer built-in Spark functions like `when()`, `col()`, `concat()`, `upper()`, etc. These are optimized and executed natively.

---

## ‚úÖ 7. Tune Spark Configurations

Fine-tune Spark properties based on workload:

```python
spark.conf.set("spark.sql.shuffle.partitions", "200")
spark.conf.set("spark.executor.memory", "4g")
spark.conf.set("spark.executor.cores", "4")
```

---

## ‚úÖ 8. Skew Handling Techniques

When data is unevenly distributed (skewed), a few partitions may slow down the entire job.

### Solutions:

* Use **salting** (add a random key to distribute skewed keys)
* Use **broadcast joins** to avoid shuffles
* Filter out skewed keys and process them separately

---

## ‚úÖ Summary Table

| Technique               | Purpose                                  |
| ----------------------- | ---------------------------------------- |
| `repartition()`         | Increase number of partitions            |
| `coalesce()`            | Reduce partitions without full shuffle   |
| `broadcast()`           | Optimize joins with small tables         |
| `cache()` / `persist()` | Store intermediate results               |
| Column pruning          | Read only necessary columns from disk    |
| Predicate pushdown      | Filter data at storage level             |
| Avoid UDFs              | Use optimized built-in functions         |
| Tune configs            | Control memory, parallelism, and shuffle |
| Handle skew             | Use salting, broadcast, or custom logic  |

---
Absolutely! Here's a clear and professional breakdown of the advanced Spark performance concepts you've mentioned, all written in **clean Markdown** format suitable for technical portfolios, documentation, or study notes.

---

# ‚öôÔ∏è PySpark Performance Essentials

---

## üîÅ Repartition vs Coalesce

### üîπ `repartition()`

* **Used to increase** the number of partitions
* **Triggers a full shuffle** of data across the cluster
* **Ensures even distribution** of records
* Suitable when you're preparing for wide transformations (e.g., joins)

```python
df_repart = df.repartition(100)
```

### üîπ `coalesce()`

* **Used to reduce** the number of partitions
* **Avoids full shuffle** (uses narrow transformation)
* More efficient than `repartition()` for downsizing
* Best used before writing data (e.g., fewer output files)

```python
df_coal = df.coalesce(10)
```

> ‚úÖ **Rule of Thumb**:
> Use `repartition()` to **increase partitions**, and `coalesce()` to **reduce them** before saving.

---

## üß† Caching and Persistence

### üîπ cache()

* Stores the DataFrame in **memory**.
* Ideal for reusing the same dataset multiple times in your pipeline.
* Easy to apply: `df.cache()`

### üîπ persist()

* More flexible: stores data in **memory, disk, or both**.
* Use when data doesn‚Äôt fit into memory completely.

```python
from pyspark import StorageLevel
df.persist(StorageLevel.MEMORY_AND_DISK)
```
---

## ‚öôÔ∏è Catalyst Optimizer and Tungsten Engine

### üîπ Catalyst Optimizer

* Spark‚Äôs **rule-based and cost-based** query optimizer for DataFrames and SQL.
* Optimizes logical query plans via:

  * Predicate pushdown
  * Constant folding
  * Null propagation
  * Filter reordering

**Example:**

```python
df.select("name").filter("age > 30")
```

Catalyst rearranges operations for optimal performance.

### üîπ Tungsten Execution Engine

* Spark‚Äôs **physical execution engine** for memory management and binary processing.
* Includes:

  * Whole-stage code generation
  * Off-heap memory use
  * Cache-friendly execution

> ‚ö†Ô∏è Custom UDFs **disable Catalyst and Tungsten** benefits ‚Äì avoid them unless necessary.

---

## üß± Data Skew Handling

**Data Skew** occurs when certain keys appear more frequently than others, causing uneven load and slow performance in partitions.

### üîπ How to Detect Skew:

* Long stage durations
* Executors stuck on a few tasks
* Uneven partition sizes

### üîß Solutions:

1. **Salting Technique**

Add a random prefix to skewed keys to spread data more evenly.

```python
from pyspark.sql.functions import rand

df = df.withColumn("salt", (rand() * 10).cast("int"))
```

2. **Broadcast Join**

Use when one side of the join is small.

```python
df_large.join(broadcast(df_small), "key")
```

3. **Skewed Key Isolation**

Process skewed keys separately, then merge with the main data.

4. **Skew-aware Aggregation**

Use approximate aggregations or filter out high-frequency keys.

---

## üì¶ Best Practices for Large Datasets

### ‚úÖ General Tips

| Area               | Practice                                                                |
| ------------------ | ----------------------------------------------------------------------- |
| File Format        | Use **Parquet** or **ORC** for storage efficiency                       |
| Read Efficiency    | Select only needed columns, apply filters early                         |
| Partition Strategy | Tune number of partitions (based on data size and executor count)       |
| Caching            | Cache reused data intelligently                                         |
| Join Strategy      | Use **broadcast joins** when applicable                                 |
| Memory Management  | Use `persist()` for large iterative jobs                                |
| Avoid UDFs         | Use built-in functions like `when()`, `col()`, `regexp_extract()`       |
| Compression        | Use `snappy` with Parquet or ORC for faster reads                       |
| Monitoring         | Use Spark UI to identify stages with skew, long tasks, or memory issues |

---


