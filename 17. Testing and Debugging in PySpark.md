
---

## üß™ 17. Testing and Debugging in PySpark

Testing and debugging are crucial in PySpark, especially when working with distributed environments, large datasets, and pipelines involving multiple data sources (like S3, Snowflake, Web APIs).

This section covers:

* Unit testing strategies
* Debugging PySpark locally and in clusters
* Effective logging
* Performance tools like `.explain()` and Spark UI

---

### ‚úÖ Unit Testing PySpark Code

#### üîπ Why test PySpark code?

* Ensure data quality and schema integrity
* Prevent regressions when pipelines evolve
* Speed up development with confidence
* Automate deployment pipelines (CI/CD)

#### üîπ Frameworks Used

* [`pytest`](https://docs.pytest.org) ‚Äì lightweight and widely used Python test framework
* `unittest` ‚Äì Python‚Äôs built-in testing framework

---

#### üî∏ Setting Up a Local SparkSession for Testing

You can mock a small Spark session for local unit testing:

```python
import pytest
from pyspark.sql import SparkSession

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder \
        .appName("TestSession") \
        .master("local[2]") \
        .getOrCreate()
```

#### üî∏ Sample PySpark Test

```python
def test_filter_adults(spark):
    data = [("Alice", 30), ("Bob", 15)]
    df = spark.createDataFrame(data, ["name", "age"])

    result = df.filter(df.age >= 18).collect()
    assert len(result) == 1
    assert result[0]["name"] == "Alice"
```

You can run this with:

```bash
pytest test_my_module.py
```

---

### üñ•Ô∏è Local vs Cluster Debugging

#### ‚úÖ Local Debugging (using `local[*]` master)

Pros:

* Easy to test on your laptop
* Faster iteration cycle
* Ideal for unit testing, schema validation, logic verification

Cons:

* Doesn‚Äôt simulate cluster behavior (e.g., shuffles, memory issues, data skew)

---

#### ‚úÖ Cluster Debugging (EMR, Databricks, YARN, Kubernetes)

Use for:

* Production-scale data
* Performance debugging (e.g., skew, spill, OOM)
* Monitoring memory, GC, stages

Strategies:

* Use `spark-submit` with `--deploy-mode cluster` to reproduce the issue
* Check **Spark History Server** or **Databricks UI** for logs and stages

---

### üßæ Logging Strategies in PySpark

Use the built-in `logging` module instead of `print()` for better control.

```python
import logging

logger = logging.getLogger("my_logger")
logger.setLevel(logging.INFO)

logger.info("Job started")
logger.warning("Missing values found in column X")
```

You can also configure logs in `log4j.properties` in EMR or Databricks to route logs to CloudWatch/S3.

---

### üîç Using `.explain()` and Spark UI

#### üîπ `.explain()` for Plan Debugging

Use `.explain()` to see the physical and logical execution plan.

```python
df.groupBy("dept").count().explain(True)
```

Output shows:

* Logical plan
* Optimized plan (Catalyst optimizer)
* Physical plan (what will actually run)

Use this to:

* Identify unnecessary shuffles
* Verify partition usage
* Detect broadcast joins vs shuffle joins

---

#### üîπ Spark UI

Accessible when running on:

* Localhost (`http://localhost:4040`)
* EMR: Spark History Server
* Databricks: Spark Jobs tab

Tracks:

* Jobs and Stages
* Time taken by each task
* Shuffle and GC metrics
* Skewed stages (look for long-running tasks)

---

### ‚úÖ Summary

| Area                | Tool / Technique                      | Purpose                                     |
| ------------------- | ------------------------------------- | ------------------------------------------- |
| Unit Testing        | `pytest`, `unittest`                  | Validate logic on small data samples        |
| Debugging (local)   | `local[*]` mode, `.show()`, `print()` | Quick testing before submitting to cluster  |
| Debugging (cluster) | Spark UI, History Server              | Analyze job execution, shuffles, failures   |
| Logging             | `logging` module, log4j               | Maintain logs across job stages and retries |
| Performance Tuning  | `.explain()`                          | Understand plan and optimize Spark code     |

---

### üöÄ Best Practices

* **Write unit tests** for all data transformation functions.
* Use **sample data** in local mode to test your logic.
* **Avoid hardcoded file paths** and use configs for flexibility.
* Always use `.explain()` when jobs are slow or unexpectedly large.
* Monitor **job logs and stages** for data skew or shuffles.

---
