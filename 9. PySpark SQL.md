
---

### ðŸ§¾ 9. PySpark SQL

---

#### ðŸ”¹ What is PySpark SQL?

**PySpark SQL** is a module in Apache Spark that allows you to execute SQL queries on structured data using the Spark engine. It supports both **SQL syntax** and **DataFrame DSL (Domain Specific Language)**. You can seamlessly switch between SQL-style queries and functional programming.

---

### âœ… Creating Temporary Views

To run SQL queries on a DataFrame, you must first register it as a **temporary view** or **global temporary view**.

```python
df.createOrReplaceTempView("employees")
```

This creates a view named `employees` accessible within the current Spark session.

```python
df.createGlobalTempView("employees_global")
```

This creates a **global** view available across sessions under the `global_temp` database.

---

### âœ… Writing and Executing SQL Queries

Once the view is registered, you can run SQL queries using `spark.sql()`.

```python
spark.sql("SELECT name, salary FROM employees WHERE salary > 50000").show()
```

This behaves just like running SQL in a database, and the result is a new DataFrame.

---

### ðŸ”„ SQL vs DSL (DataFrame API) Comparison

You can achieve the same results using both SQL and DSL. Hereâ€™s a side-by-side comparison:

| Operation        | SQL Query                                         | DataFrame API                      |
| ---------------- | ------------------------------------------------- | ---------------------------------- |
| Select columns   | `SELECT name, age FROM people`                    | `df.select("name", "age")`         |
| Filtering rows   | `SELECT * FROM people WHERE age > 30`             | `df.filter(df["age"] > 30)`        |
| Grouping and agg | `SELECT dept, AVG(salary) FROM emp GROUP BY dept` | `df.groupBy("dept").avg("salary")` |
| Ordering         | `SELECT * FROM emp ORDER BY salary DESC`          | `df.orderBy(df["salary"].desc())`  |

**When to use what:**

* Use **SQL** for users familiar with database queries.
* Use **DSL** for tighter integration with PySpark functions, chaining, and more complex pipelines.

---

### ðŸ§  Registering UDFs for SQL Use

If you have a custom Python function and want to use it inside SQL queries, register it as a **UDF**.

```python
def upper_case(text):
    return text.upper()

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

spark.udf.register("to_upper", upper_case, StringType())

# Use in SQL
spark.sql("SELECT to_upper(name) FROM employees").show()
```

This allows your Python logic to be directly embedded in SQL statements.

---

### âœ… Summary

* PySpark SQL allows querying structured data using standard SQL syntax.
* Use `.createOrReplaceTempView()` to register a DataFrame as a SQL view.
* SQL queries return DataFrames and can be seamlessly combined with DSL code.
* You can register and use UDFs in SQL queries just like in regular Spark transformations.
* Ideal for data analysts and engineers working in hybrid SQLâ€“code environments.

---
