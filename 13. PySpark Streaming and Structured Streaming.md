
---

### 🔁 13. PySpark Streaming and Structured Streaming

---

#### 🔹 What is Streaming in PySpark?

Streaming in Spark allows you to process **real-time data** continuously as it arrives. This is useful for applications like:

* Real-time dashboards
* Log or sensor data processing
* Fraud detection
* Event-driven pipelines (clickstreams, IoT, etc.)

Apache Spark supports **two types of streaming:**

1. **DStream-based Spark Streaming** (older API – based on RDDs)
2. **Structured Streaming** (modern API – based on DataFrames)

👉 **Structured Streaming** is now the standard and recommended approach.

---

### 📦 Structured Streaming Overview

Structured Streaming treats real-time data as an **unbounded table** and processes it using familiar DataFrame operations like `select()`, `filter()`, `groupBy()`, etc.

You write batch-like code and Spark automatically handles **continuous updates**, **checkpointing**, and **state management**.

---

### ✅ Reading Streaming Data

You can read streaming data from various sources such as Kafka, socket, or files.

**Example: Read from a socket**

```python
df = spark.readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()
```

**Example: Read from Kafka**

```python
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "my-topic") \
    .load()
```

---

### ✅ Performing Transformations

Once you’ve read the stream, you can use normal DataFrame operations.

```python
from pyspark.sql.functions import split

# Assume input is lines of text
words = df.select(split(df["value"], " ").alias("words"))
```

You can also aggregate, join, filter, or apply ML models — just like with batch data.

---

### ✅ Writing Streaming Output

After processing, the results must be written to a sink continuously.

Supported output modes:

* **append** – new rows only
* **complete** – full result table
* **update** – changed rows only

**Example: Write to console**

```python
query = words.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
```

**Other sinks:** Kafka, files, memory tables, foreach writers, databases.

---

### 🧠 Trigger Types

You can control when the micro-batches run using **triggers**:

* `trigger(processingTime='5 seconds')` – run every 5 seconds
* `trigger(once=True)` – run just once (batch-like)
* `trigger(continuous='1 second')` – low-latency streaming (experimental)

---

### ✅ Checkpointing & Fault Tolerance

To recover from failures, Spark requires **checkpointing**:

```python
.writeStream
.option("checkpointLocation", "/path/to/checkpoint")
```

Without checkpoints, the job can’t recover from crashes or restarts.

---

### 🔍 Windowed Aggregations

To group streaming data by time (e.g., 5-minute window), use `window()`:

```python
from pyspark.sql.functions import window

df.groupBy(window(df["timestamp"], "5 minutes")).count()
```

Useful for sliding time windows, such as:

* Count of events per 10-minute interval
* Aggregates of sensor data every 1 hour

---

### ✅ Use Cases of Structured Streaming

* **Fraud detection** in financial transactions
* **Real-time monitoring** of server logs
* **IoT sensor pipelines**
* **Social media trend tracking**
* **Real-time recommendation systems**

---

### 🔄 DStreams vs Structured Streaming

| Feature           | DStreams (Old API) | Structured Streaming        |
| ----------------- | ------------------ | --------------------------- |
| API Base          | RDD                | DataFrame                   |
| Performance       | Limited            | Optimized via Catalyst      |
| Fault Tolerance   | Yes                | Yes                         |
| Output Modes      | Limited            | Append, Update, Complete    |
| Windowing         | Manual             | Simplified with `window()`  |
| Supported Formats | Fewer              | Kafka, files, sockets, etc. |
| Status in Spark   | Legacy             | Actively developed          |

---

### ✅ Summary

* **Structured Streaming** is the modern, scalable, and optimized API for real-time data.
* You can use familiar DataFrame syntax to process streaming data from Kafka, sockets, and files.
* Supports micro-batch processing with fault tolerance and triggers.
* Use `checkpointLocation` to ensure state recovery.
* Ideal for building end-to-end **real-time data pipelines** in production.

---

